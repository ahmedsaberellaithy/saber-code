# Saber Code CLI Configuration Example
# Copy this file to .env and customize as needed

# ═══════════════════════════════════════════════════════════
#  PRIMARY AI MODEL CONFIGURATION
# ═══════════════════════════════════════════════════════════

# Recommended Model (Best Performance)
# See MODEL_COMPARISON.md for detailed benchmarks and comparisons
SABER_CODE_MODEL=qwen2.5-coder:32b-instruct

# Alternative Models:
# 
# For smaller laptops (8GB+ RAM):
# SABER_CODE_MODEL=deepseek-coder-v2:16b
#
# For budget/low-end systems (6GB+ RAM):
# SABER_CODE_MODEL=qwen2.5-coder:7b-instruct
#
# Original default (may produce template responses):
# SABER_CODE_MODEL=codellama:13b
#
# Good general + code (better than codellama):
# SABER_CODE_MODEL=mistral


# ═══════════════════════════════════════════════════════════
#  OLLAMA SERVER CONFIGURATION
# ═══════════════════════════════════════════════════════════

# Ollama API Base URL (local server)
SABER_CODE_BASE_URL=http://localhost:11434

# API Timeout in milliseconds
# Recommendations:
#   - Chat/Quick tasks: 60000 (1 minute)
#   - Plan creation: 120000 (2 minutes)
#   - Complex analysis: 180000 (3 minutes)
SABER_CODE_TIMEOUT=120000


# ═══════════════════════════════════════════════════════════
#  CONTEXT & TOKEN MANAGEMENT
# ═══════════════════════════════════════════════════════════

# Maximum tokens to send in context
# Model limits:
#   - qwen2.5-coder:32b-instruct: 32000 (128000 capable)
#   - deepseek-coder-v2:16b: 128000
#   - codellama:13b: 16000
SABER_CODE_MAX_TOKENS=32000

# Maximum files to keep in context
# More files = better context but uses more tokens
SABER_CODE_MAX_FILES=20

# Maximum conversation messages to retain
# Balance between context and token usage
SABER_CODE_MAX_CONVERSATION=50


# ═══════════════════════════════════════════════════════════
#  DEBUGGING & LOGGING
# ═══════════════════════════════════════════════════════════

# Enable verbose debug output
# Set to 1 to see:
#   - API requests and responses
#   - Token counts and context usage
#   - Timing information
#   - Prompt construction
# Set to 0 for clean output (default)
DEBUG=0


# ═══════════════════════════════════════════════════════════
#  PROJECT STORAGE (Advanced - Usually Don't Need to Change)
# ═══════════════════════════════════════════════════════════

# Directory for chat history and knowledge base
# SABER_CODE_HISTORY_DIR=.saber_code_history

# Directory for saved plans
# SABER_CODE_PLANS_DIR=_saber_code_plans


# ═══════════════════════════════════════════════════════════
#  QUICK START GUIDE
# ═══════════════════════════════════════════════════════════
#
# 1. Copy this file:
#    $ cp .env.example .env
#
# 2. Install Ollama:
#    $ brew install ollama  # macOS
#    # or visit https://ollama.com/download
#
# 3. Pull recommended model:
#    $ ollama pull qwen2.5-coder:32b-instruct
#
# 4. Start Ollama server:
#    $ ollama serve
#
# 5. Test Saber Code:
#    $ node cli.js models
#    $ node cli.js chat
#
# 6. Read documentation:
#    $ cat START_HERE.md
#    $ cat MODEL_COMPARISON.md
#
# ═══════════════════════════════════════════════════════════


# ═══════════════════════════════════════════════════════════
#  MODEL SELECTION GUIDE
# ═══════════════════════════════════════════════════════════
#
# Choose based on your hardware:
#
# 32GB+ RAM, M1/M2/M3 Mac or high-end PC:
#   → qwen2.5-coder:32b-instruct (RECOMMENDED)
#     Best quality, excellent instruction following
#
# 16GB RAM, modern laptop:
#   → qwen2.5-coder:32b-instruct (works, might be slower)
#   → deepseek-coder-v2:16b (faster alternative)
#
# 8-12GB RAM, mid-range laptop:
#   → deepseek-coder-v2:16b (good balance)
#   → qwen2.5-coder:7b-instruct (lighter option)
#
# 6-8GB RAM, older/budget laptop:
#   → qwen2.5-coder:7b-instruct
#   → codellama:13b (fallback, but lower quality)
#
# See MODEL_COMPARISON.md for detailed benchmarks!
#
# ═══════════════════════════════════════════════════════════


# ═══════════════════════════════════════════════════════════
#  TROUBLESHOOTING
# ═══════════════════════════════════════════════════════════
#
# Issue: "Ollama not responding"
# Fix: Ensure Ollama is running
#   $ ollama serve
#
# Issue: "Model not found"
# Fix: Pull the model first
#   $ ollama pull qwen2.5-coder:32b-instruct
#
# Issue: "Plan returns template/placeholders"
# Fix: Use qwen2.5-coder:32b-instruct (better than codellama)
#
# Issue: "Timeout errors"
# Fix: Increase SABER_CODE_TIMEOUT to 180000 or higher
#
# Issue: "Out of memory"
# Fix: Use smaller model (deepseek-coder-v2:16b or qwen2.5-coder:7b)
#
# ═══════════════════════════════════════════════════════════
